{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning with pytorch(60分中简要教程)\n",
    "## 什么是pytorch\n",
    "+ 可以用来替代Numpty，但比Numpty增加了GPU的支持\n",
    "+ 深度学习框架，提供了最大化的灵活性和性能\n",
    "\n",
    "### Tensor(张量)\n",
    "    Tensor类似于Numpty中的ndarray，但是增加了对GPU的支持。 使用tensor，首先导入torch模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "print(torch.__version__) #打印torch版本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成一个5x3的矩阵，为初始化:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, -0.0000e+00, 2.5777e+33],\n",
      "        [8.5920e+09, 1.1210e-44, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成一个5x3的矩阵，随机初始化:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3434, 0.4575, 0.5652],\n",
       "        [0.3717, 0.7905, 0.1333],\n",
       "        [0.8853, 0.1139, 0.3791],\n",
       "        [0.3014, 0.8595, 0.8263],\n",
       "        [0.2763, 0.8001, 0.9045]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成一个5x3的矩阵，所有元素初始化为零，数据类型为long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直接从数据生成tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3000, 3.4000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2.3, 3.4])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于已经生成的Tensor生成新的Tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0.1396, 0.4407, 0.0209],\n",
      "        [0.4998, 0.3605, 0.5077],\n",
      "        [0.1640, 0.1786, 0.8589],\n",
      "        [0.5465, 0.2757, 0.2669],\n",
      "        [0.9443, 0.6355, 0.7412]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)  #新生成的x，继承了原先x的所有属性，包括dtype和device\n",
    "print(x)\n",
    "\n",
    "x = torch.rand_like(x,dtype=torch.float)  #基于x生成新的tensor y\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取tensor的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算操作\n",
    "#### 加法1(+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1089, 1.2746, 1.5135],\n",
      "        [1.3234, 1.2899, 1.4865],\n",
      "        [1.5315, 1.1465, 1.1088],\n",
      "        [1.3619, 1.0314, 1.0876],\n",
      "        [1.2284, 1.4970, 1.9814]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加法2(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1089, 1.2746, 1.5135],\n",
      "        [1.3234, 1.2899, 1.4865],\n",
      "        [1.5315, 1.1465, 1.1088],\n",
      "        [1.3619, 1.0314, 1.0876],\n",
      "        [1.2284, 1.4970, 1.9814]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add 方法提供了一个out参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1089, 1.2746, 1.5135],\n",
      "        [1.3234, 1.2899, 1.4865],\n",
      "        [1.5315, 1.1465, 1.1088],\n",
      "        [1.3619, 1.0314, 1.0876],\n",
      "        [1.2284, 1.4970, 1.9814]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add: in-place方法 (在原内存替换)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1089, 1.2746, 1.5135],\n",
      "        [1.3234, 1.2899, 1.4865],\n",
      "        [1.5315, 1.1465, 1.1088],\n",
      "        [1.3619, 1.0314, 1.0876],\n",
      "        [1.2284, 1.4970, 1.9814]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)   #in-place，修改y的值\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor的数据截取，类似于Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3234, 1.2899, 1.4865],\n",
      "        [1.5315, 1.1465, 1.1088],\n",
      "        [1.3619, 1.0314, 1.0876],\n",
      "        [1.2284, 1.4970, 1.9814]])\n"
     ]
    }
   ],
   "source": [
    "print(y[1:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改tensor的size,Numpy用reshape函数，pytorch用view函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view((-1, 8))\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是单个元素的tensor，可以通过item函数,获取到python标量(scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8588])\n",
      "0.8587820529937744\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与Numpy之间的转换\n",
    "    torch的tensor和Numpy的ndarray共享的是同一块内存(在cpu模式下)，所以两者是相互影响的。\n",
    "#### torch的tensor转换到Numpy的Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "\n",
    "a.add_(1)  #tensor a 和 numpy b 共享同一块内存，所以a修改后，b的值也相应修改\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy的Array转换到Torch的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA\n",
    "前文说过，Torch的Tensor与Numpy Array的一个区别是可以在GPU上存储，有两种方式：\n",
    "* tensor创建的时候，就直接申请在cuda device\n",
    "* tensor通过to函数，放到cuda上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is invalidate!\n"
     ]
    }
   ],
   "source": [
    "#判断是否支持cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    y = torch.ones(4,4,device=device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to('cpu', dtype=torch.float32))\n",
    "else:\n",
    "    print('cuda is invalidate!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd(自动求导)\n",
    "pytorch的核心包是autograd,autograd包提供了tensor自动求导功能。\n",
    "#### Tensor\n",
    "    torch.tensor是autograd的核心类，如果将tensor的requires_grad参数设置为True，tensor将会记录所有操作。当计算完成后，tensor调用backward()函数后，会自动计算出tensor的gradient(梯度),并保存到tensor的grad参数下。\n",
    "    如果需要停止梯度计算的话，调用tensor的detach()函数，从计算图中剥离。另外也可以通过将代码块放在 with torch.no_grad()下，这样代码块中的tensor都不会计算梯度。\n",
    "    tensor还有一个重要的参数grad_fn。grad_fn是Function函数的实现，每个tensor都有grad_fn，用户创建的tensor的grad_fn为None。\n",
    "    如果想要计算导数，可以调用tensor的backward()函数，如果tensor是一个标量（scalar)，backward()函数不需要传入任何值；如果tensor包含多个元素，backward()需要设置gradients，参数的size与tensor相同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
